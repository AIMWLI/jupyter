{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "  table td {text-align: left  !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "  table td {text-align: left  !important;}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 介绍\n",
    "&emsp;&emsp;支持向量机(SVM，也称为支持向量网络)虽然诞生只有短短的二十多年，但是自一诞生便由于它良好的分类性能席卷了机器学习领域，\n",
    "并牢牢压制了神经网络领域好多年，是机器学习中获得关注最多的算法没有之一。它源于统计学习理论，\n",
    "是我们除了集成算法之外，接触的第一个强学习器。  \n",
    "&emsp;&emsp;SVM是一个二元分类算法，线性分类和非线性分类都支持。经过演进，现在也可以支持多元分类，同时经过扩展，也能应用于回归问题。\n",
    "\n",
    "**从功能来看**, svm几乎覆盖了所有机器学习算法的功能:\n",
    "\n",
    "分类  | 功能\n",
    "---  | ---\n",
    "有监督学习|线性二分类与多分类（Linear Support Vector Classiﬁcation）<br> 非线性二分类与多分类（Support Vector Classiﬁcation, SVC）<br> 普通连续型变量的回归（Support Vector Regression）<br> 概率型连续变量的回归（Bayesian SVM）\n",
    "无监督学习|支持向量聚类（Support Vector Clustering，SVC）<br> 异常值检测（One-class SVM）\n",
    "半监督学习|转导支持向量机（Transductive Support Vector Machines，TSVM）\n",
    "\n",
    "**从分类的效果来看**,SVM无论是在线性分类还是非线性分类中,都是明星般的存在:\n",
    "![分类算法比较](../images/算法分类比较.png)\n",
    "\n",
    "**从实际应用来看**，SVM在各种实际问题中都表现非常优秀。它在手写识别数字和人脸识别中应用广泛，在文本和超文本的分类中举足轻重，\n",
    "因为SVM可以大量减少标准归纳（standard inductive）和转换设置（transductive\n",
    "settings）中对标记训练实例的需求。同时，SVM也被用来执行图像的分类，并用于图像分割系统。实验结果表\n",
    "明，在仅仅三到四轮相关反馈之后，SVM就能实现比传统的查询细化方案（query reﬁnement schemes）高出一大截的搜索精度。除此之外，\n",
    "生物学和许多其他科学都是SVM的青睐者，SVM现在已经广泛被用于蛋白质分类，现在化合物分类的业界平均水平可以达到90%以上的准确率。\n",
    "在生物科学的尖端研究中，人们还使用支持向量机来识 别用于模型预测的各种特征，以找出各种基因表现结果的影响因素。\n",
    "\n",
    "**从学术的角度来看**，SVM是最接近深度学习的机器学习算法。线性SVM可以看成是神经网络的单个神经元（虽然损失函数与神经网络不同），\n",
    "非线性的SVM则与两层的神经网络相当，非线性的SVM中如果添加多个核函数，则可以模仿多层的神经网络。\n",
    "\n",
    "# SVM分类器是如何工作的\n",
    "支持向量机所作的事情其实非常容易理解。先来看看下面这一组数据的分布，这是一组两种标签的数据，两种标签分别由圆和方块代表。\n",
    "支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，\n",
    "尤其是在未知数据集上的分类误差（泛化误差）尽量小。\n",
    "![svm1](../images/svm1.png)\n",
    "关键概念:超平面\n",
    ">在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。如果数据空间本身是三维的，\n",
    "则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。\n",
    "在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就说这个超平面是数据的“决策边界”。\n",
    "\n",
    "决策边界一侧的所有点在分类为属于一个类，而另一侧的所有点分类属于另一个类。如果我们能够找出决策边界，分类问题就可以变成探讨每个样本对于决策\n",
    "边界而言的相对位置。比如上面的数据分布，我们很容易就可以在方块和圆的中间画出一条线，并让所有落在直线左边的样本被分类为方块，在直线右边的样本\n",
    "被分类为圆。如果把数据当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。\n",
    "对于一个数据集来说，让训练误差为0的决策边界可以有无数条。\n",
    "![svm2](../images/svm2.png)\n",
    "但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说， 我们有$B_1$和$B_2$两条可能的决策边界。\n",
    "我们可以把决策边界$B_1$向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是$b_{11}$和$b_{12}$，并且我们将\n",
    "原始的决策边界移动到$b_{11}$和$b_{12}$的中间，确保\n",
    "到$b_{11}$和$b_{12}$的距离相等。在$b_{11}$和$b_{12}$中间的距离，叫做这条决策边界的间隔(margin)，通常记作d。\n",
    "\n",
    "为了简便，我们称$b_{11}$和$b_{12}$为“虚线超平面”，在其他博客或教材中可能有着其他的称呼，但大家知道是这两个超平面是由原来的决策边界向两边\n",
    "移动，直到碰到距离原来的决策边界最近的样本后停下而形成的超平面就可以了。\n",
    "对$B_2$也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判断为方块，两条决策边界在\n",
    "现在的数据集上的训练误差都是0，没有一个样本被分错。\n",
    "![svm3](../images/svm3.png)\n",
    "我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于$B_1$\n",
    "而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于$B_2$而言，却有三个方块被误分类为圆，有两个圆被误分类成了方块，这条决\n",
    "策边界上的泛化误差就远远大于$B_1$了。这个例子表现出，拥有更大间隔的决策边界在分类中的泛化误差更小，这一点可以由结构风险最小化定律来\n",
    "证明（SRM）。如果间隔很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。所以我们在找寻决策边界的时候，希望边际越大越好。\n",
    "![svm4](../images/svm4.png)\n",
    "支持向量机，就是通过找出间隔最大的决策边界，来对数据进行分类的分类器。也因此，支持向量分类器又叫做最大间隔分类器。这个过程在二维平面中看起来十分简单，但将上述过程使用数学表达出来，就不是一件简单的事情了。\n",
    "\n",
    "# SVM模型目标函数\n",
    "假设现在数据中总计有N个训练样本，每个训练样本i可以被表示为($x_i$,$y_i$)(i=1,2,...,N)，其中$x_i$是$(x_{1i},x_{ni},...,x_{ni})^T$\n",
    "这样的一个特征向量，每个样本总共含有n个特征。 二分类标签y的取值是{-1, 1}。\n",
    "如果n等于2，则第i个样本为$(x_{1i},x_{2i},y_{i})^T$，分别由我们的特征向量和标签组成。此时我们可以在二维平面上，以$x_2$横坐标，$x_1$为\n",
    "纵坐标，y为颜色，来可视化我们所有的N个样本：\n",
    "![svm5](../images/svm5.png)\n",
    "我们让所有紫色点的标签为1，红色点的标签为-1。我们要在这个数据集上寻找一个决策边界，在二维平面上，决策边界（超平面）就是一条直线。二维平面上的任意一条线可以被表示为：\n",
    "$$x_1=ax_2+b$$\n",
    "我们将此表达式变换一下:\n",
    "\n",
    "$$0=ax_2-x_1+b$$\n",
    "$$0=\\left[ \\begin{matrix} a &-1 \\end{matrix} \\right]*\\left[ \\begin{matrix} x_2\\\\x_1\\end{matrix} \\right]+b$$\n",
    "$$0=\\boldsymbol{ω^Tx} + \\boldsymbol b $$\n",
    "\n",
    "在一组数据下，给定固定的ω和b，这个式子就可以是一条固定直线，在ω和b不确定的状况下，这个表达式$0=ω^Tx + b$就可以代表平面上的任意一条直线。\n",
    "在SVM中，我们就使用这个表达式来表示我们的决策边界。我们的目标是求解能够让边际d最大化的决策边界，所以我们要求解参数向量ω和截距b。\n",
    "\n",
    "# 损失函数\n",
    "如果在决策边界上任意取两个点$x_a,x_b$,，并带入决策边界的表达式，则有：\n",
    "$$0=ω^Tx_a + b$$\n",
    "$$0=ω^Tx_b + b$$\n",
    "\n",
    "将两式相减，可以得到：\n",
    "$$0=ω^T(x_a-x_b)$$\n",
    "\n",
    "一个列向量的转至乘以另一个列向量，可以获得两个向量的点积(dot product)，表示为$<ω^T\\cdot(x_a-x_b)>$。两个向量的点积为0表示两个向量的方向式互相垂直的。 $x_a$与$x_b$是一条直线上的两个点，相减后的得到的向量方向是由\n",
    "$x_b$指向$x_a$，所以$(x_a-x_b)$的方向是平行于他们所在的直线——我们的决策边界的。而ω与$(x_a-x_b)$相互垂直，所以参数向量ω的方向必然是垂直于我们的决策边界。\n",
    "![svm6](../images/svm6.png)\n",
    "\n",
    "此时，我们有了我们的决策边界。对于任意一个紫色的点$x_p$,有:\n",
    "$$ω^Tx_p + b=p$$\n",
    "\n",
    "我们假设紫色的点所代表的标签y是1，所以p>0。同样的，对于任意一个红色的点$x_r$有：\n",
    "$$ω^Tx_r + b=r$$\n",
    "\n",
    "我们假设红色的点所代表的标签y是-1，所以r<0。如果我们有新的测试数据$x_t$,则$x_t$的标签可以根据以下式子来判定:\n",
    "\n",
    "$$y=\\begin{cases} 1 & ω^Tx_t + b > 0 \\\\ -1 & ω^Tx_t + b < 0 \\end{cases}$$\n",
    "\n",
    "\n",
    "我们之前说过，决策边界的两边要有两个超平面，这两个超平面在二维空间中就是两条平行线（就是我们的虚线超平面），而他们之间的距离就是我们的边际。\n",
    "而决策边界位于这两条线的中间，所以这两条平行线必然是对称的。\n",
    "![svm7](../images/svm7.png)\n",
    "\n",
    "我们将这两条平行线表示为：\n",
    "$$ω^Tx + b=k,ω^Tx + b=-k$$\n",
    "两个表达式同时除以k，则可以得到：\n",
    "$$ω^Tx + b=1,ω^Tx + b=-1$$\n",
    "这就是我们平行于决策边界的两条线的表达式，表达式两边的1和-1分别表示了两条平行于决策边界的虚线到决策 边界的相对距离。此时，\n",
    "我们可以让这两条线分别过两类数据中距离我们的决策边界最近的点，这些点就被称为“支持向量”，而决策边界永远在这两条线的中间，\n",
    "所以可以被调整。我们令紫色类的点为$x_p$，红色类的点为$x_r$， 则我们可以得到：\n",
    "$$ω^Tx_p + b=1,ω^Tx_r + b=-1$$\n",
    "两个式子相减，则有：\n",
    "$$ω^T(x_p-x_r)=2$$\n",
    "\n",
    "如图所示，$(x_p-x_r)$可表示为两点之间的连线，而我们的边际d是平行于ω的，所以我们现在相当于是得到了三角形中的斜边，\n",
    "并且知道一条直角边的方向。在线性代数中，我们有如下数学性质：\n",
    "线性代数中模长的运用  \n",
    ">向量b除以自身的模长${||b||}$可以得到b方向上的单位向量.  \n",
    ">向量a乘以b方向上的单位向量,可以得到向量a在向量b方向上的投影的长度.\n",
    "\n",
    "所以，我们另上述式子两边同时除以${||ω||}$，则可以得到：\n",
    "$$\\frac{ω^T(x_p-x_r)}{||ω||}=\\frac{2}{||ω||}$$\n",
    "$$\\therefore d = \\frac{2}{||ω||}$$\n",
    "\n",
    "我们的目标是最大化边界所对应的决策边界，要最大化d，就求解ω的最小值。极值问题可以相互转化，我们可以把求解ω的最小值转化为，求解以下函数的最小值：\n",
    "$$f(ω)=\\frac{||ω||^2}{2}$$\n",
    "我们的两条虚线表示的超平面，是数据边缘所在的点。所以对于任意样本i，我们可以把决策函数写作：\n",
    "$$ω^Tx_i + b \\geq 1  \\quad if \\, y_i=1$$\n",
    "$$ω^Tx_i + b \\leq -1  \\quad if \\, y_i=-1$$\n",
    "\n",
    "整理一下，我们可以把两个式子整合成：\n",
    "$$y_i(ω^Tx_i + b) \\geq 1,i=1,2,...,N$$\n",
    "\n",
    "这个式子被称为“函数间隔”。将函数间隔作为条件附加到我们的f(ω)上，我们就得到了SVM的损失函数最初形态：\n",
    "$$\\underbrace{min}_{w,b}\\frac{||ω||^2}{2}$$\n",
    "$$ subject \\, to \\, y_i(ω^Tx_i + b) \\geq 1,i=1,2,...,N$$\n",
    "\n",
    "## 补充 函数间隔与几何间隔\n",
    "函数间隔,通过观察$𝑤𝑥+𝑏$和$y$是否同号，我们判断分类是否正确，\n",
    "$$\\gamma^{'} = y(w^Tx + b)$$\n",
    "几何间隔\n",
    "$$\\gamma = \\frac{y(w^Tx + b)}{||w||_2} =  \\frac{\\gamma^{'}}{||w||_2}$$\n",
    "\n",
    "\n",
    "# 线性SVM的拉格朗日对偶函数和决策函数\n",
    "\n",
    "之前我们得到了SVM的损失函数最初形态：\n",
    "$$\\underbrace{min}_{w,b}\\frac{||ω||^2}{2}$$\n",
    "$$ s. \\, t. \\, y_i(ω^Tx_i + b) \\geq 1,i=1,2,...,N$$\n",
    "接下来要对损失函数进行求解,可以发现这个损失函数分为两部分：需要最小化的函数，以及参数求解后必须满足的约束条件。这是一个最优化问题。\n",
    "\n",
    "## 将损失函数从最初形态转换为拉格朗日乘数形态\n",
    "### 为什么要进行转换\n",
    "我们的目标是求解让损失函数最小化的ω，但其实很容易看得出来，如果||ω||为0， 必然最小了。但是，||ω||=0其实是一个无效的值，原因很简单：\n",
    "首先，我们的决策边界是$\\boldsymbol{ω^Tx} + \\boldsymbol b = 0$，如果ω为0，则这个向量里包含的所有元素都为0，那就有b = 0这个唯一值。然而，\n",
    "如果b和ω都为0，决策边界就不再是一条直线了，函数间隔$y_i(ω^Tx_i + b)$就会为0，条件中的$y_i(ω^Tx_i + b) \\geq 1$就不可能实现，所以ω不可以是一个0向量。可见，单纯\n",
    "让$f(ω)=\\frac{y(w^Tx + b)}{||w||_2}$为0,是不能求解出合理ω的,我们希望能够找出一种方式，能够让我们的条件$y_i(ω^Tx_i + b) \\geq 1$ 在计算中也被纳入考虑，\n",
    "一种业界认可的方法是使用拉格朗日乘数法(standard Lagrange multiplier method)。\n",
    "\n",
    "### 为什么可以进行转换\n",
    "我们的损失函数是二次的(quadratic)，并且我们损失函数中的约束条件在参数w和b下是线性的，求解这样的损失函数被称为\n",
    "“凸优化问题”(convex optimization problem)。拉格朗日乘数法正好可以用来解决凸优化问题，这种方法也是业界常用的，用来解决带约束条件，\n",
    "尤其是带有不等式的约束条件的函数的数学方法。首先第一步，我们需要使用拉格朗日乘数来将损失函数改写为考虑了约束条件的形式：\n",
    "$$L(w,b,α) = \\frac{1}{2}||w||^2 - \\sum\\limits_{i=1}^{m}α_i[y_i(w^Tx_i + b) - 1] \\;  (α_i \\geq 0)$$\n",
    "这是一个非常聪明而且巧妙的表达式，它被称为拉格朗日函数，其中$\\alpha_i$就叫做拉格朗日乘数。此时此刻，我们要求解的就不只有参数向量ω和截距b了，\n",
    "我们也要求解拉格朗日乘数 ，而我们的$x_i$和$y_i$都是我们已知的特征矩阵和标签。\n",
    "\n",
    "### 怎样进行转换\n",
    "拉格朗日函数分为两部分。第一部分和我们原始的损失函数一样，第二部分是我们带有不等式的约束条件。 我们希望，$L(w,b,α)$不仅能够代表\n",
    "我们原有的损失函数$f(ω)$和约束条件，还能够表示我们想要最小化损失函数来求解ω和b的意图，所以我们要先以$α$为参数，\n",
    "求解$L(w,b,α)$的最大值，再以ω和b为参数，求解$L(w,b,α)$的最小值。因此，我们的目标可以写作：\n",
    "$$\\min_{w,b}\\; \\max_{α_i \\geq 0} L(w,b,α)$$\n",
    "怎么理解这个式子呢? 首先,我们第一步先执行max,即最大化$L(w,b,α)$,那就有两种情况:   \n",
    "当$y_i(ω^Tx_i + b) > 1$,函数的第二部分$\\sum\\limits_{i=1}^{m}α_i[y_i(w^Tx_i + b) - 1]$就一定为正,式子$\\frac{1}{2}||w||_2^2$\n",
    "就要减去一个正数,此时若要最大化$L(w,b,α)$,则$α$必须取0.  \n",
    "当$y_i(ω^Tx_i + b) < 1$,函数的第二部分$\\sum\\limits_{i=1}^{m}α_i[y_i(w^Tx_i + b) - 1]$就一定为负,式子$\\frac{1}{2}||w||_2^2$\n",
    "就要减去一个负数,此时若要最大化$L(w,b,α)$,则$α$必须取正无穷.  \n",
    "若把函数第二部分当作一个惩罚项来看待，则$y_i(ω^Tx_i + b) > 1$时函数没有受到惩罚，而$y_i(ω^Tx_i + b) < 1$时函数受到了极致的惩罚，\n",
    "即加上了一个正无穷项，函数整体永远不可能取到最小值。所以第二步，我们执行min的命令，求解函数整体的最小值，我们就不能让α取到正无穷的状况出现，\n",
    "即是说永远不让$y_i(ω^Tx_i + b) < 1$的状况出现，从而实现了求解最小值的同时让约束条件被满足。\n",
    "现在,$L(w,b,α)$就是我们新的损失函数了，我们的目标是要通过先最大化，在最小化它来求解参数向量ω和截距b的值.\n",
    "\n",
    "## 将拉格朗日函数转换为拉格朗日对偶函数\n",
    "### 为什么要进行转换\n",
    "要求极值，最简单的方法还是对参数求导后让一阶导数等于0。我们先来试试看对拉格朗日函数求极值，在这里我们对参数向量w和截距b分别求偏导并且让他们等于0。\n",
    "$$\\begin{align} \n",
    "L(w,b,α)  &=  \\frac{1}{2}||w||^2 - \\sum\\limits_{i=1}^{m}α_i[y_i(wx_i + b) - 1]  \\tag{1}  \\\\& \n",
    "                = \\frac{1}{2}||w||^2 - \\sum\\limits_{i=1}^{m}(α_iy_iw\\cdot{x_i} + α_iy_ib - α_i)  \\tag{2} \\\\& \n",
    "                = \\frac{1}{2}||w||^2 - \\sum\\limits_{i=1}^{m}α_iy_iw\\cdot{x_i} - \\sum\\limits_{i=1}^{m}α_iy_ib + \\sum\\limits_{i=1}^{m}α_i \\tag{3} \\\\& \n",
    "                = \\frac{1}{2}w^Tw - \\sum\\limits_{i=1}^{m}α_iy_iw\\cdot{x_i} - \\sum\\limits_{i=1}^{m}α_iy_ib + \\sum\\limits_{i=1}^{m}α_i  \\tag{4} \\\\&\n",
    "\\end{align}$$\n",
    "求偏导\n",
    "$$\\begin{align} \n",
    "\\frac{\\partial L(w,b,α)}{\\partial w} &= \\frac{1}{2}w*2w - \\sum\\limits_{i=1}^{m}α_iy_i{x_i} \\\\&\n",
    "                                           = w - \\sum\\limits_{i=1}^{m}α_iy_i{x_i} = 0  \\;\\Rightarrow w = \\sum\\limits_{i=1}^{m}α_iy_ix_i \\tag{1} \\\\& \n",
    "\\end{align}$$\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial L(w,b,α)}{\\partial b} = \\sum\\limits_{i=1}^{m}α_iy_i = 0  \\;\\Rightarrow \\sum\\limits_{i=1}^{m}α_iy_i = 0  \\tag{2}\n",
    "\\end{align}$$\n",
    "由于两个求偏导结果中都带有未知的拉格朗日乘数α，因此我们还是无法求解出w和b，我们必须想出一种方法来求解拉格朗日乘数α。幸运地是，\n",
    "拉格朗日函数可以被转换成一种只带有α，而不带有w和b的形式，这种形式被称为拉格朗日对偶函数。在对偶函数下，我们就可以求解出拉格朗日乘数α,\n",
    "然后带入到上面推导出的(1)和(2)式 中来求解w和b。\n",
    "### 为什么可以进行转换\n",
    "对于任何一个拉格朗日函数$L(x,α)=f(x) + \\sum\\limits_{i=1}^{m}α_ih_i(x)$,都存在一个与它对应的对偶函数g(α),只带有拉格朗日乘数α作为唯一的参数.如果$L(x,α)$的最优解存在并可以表示为$\\min\\limits_xL(x,α)$,并且对偶函数的最优解也存在并且可以表示为$\\max\\limits_αg(α)$,则我们可以定义**对偶函数差异(dual gap)**,即拉格朗日函数的最优解与其对偶函数的最优解之间的差值:\n",
    "$$\\Delta = \\min_xL(x,α)-\\max_αg(α)$$\n",
    "如果$\\Delta = 0$,则称L(x,α)与其对偶函数之间存在强对偶关系(strong duality property),此时我们就可以通过求解其对偶函数的最优解来替代求解原始函数的最优解.那强对偶关系什么时候存在呢?这个拉格朗日函数必须满足KKT(Karush-Kuhn-Tucker)条件:\n",
    "$$\\frac{\\partial L}{\\partial x},\\forall_i=1,2,...,d$$\n",
    "$$h_i(x) \\leq 0,\\forall_i=1,2,...,q$$\n",
    "$$α_i,\\forall_i=1,2,...,q$$\n",
    "$$α_ih_i(x)=0,\\forall_i=1,2,...,q$$\n",
    "这里的条件是:\n",
    "* 所有参数的一阶导数必须为0，\n",
    "* 约束条件中的函数本身需要小于等于0， \n",
    "* 拉格朗日乘数需要大于等于0\n",
    "* 约束条件乘以拉格朗日乘数必须等于0，即不同i的取值下，两者之中至少有一个为0。\n",
    "当所有限制都被满足，则拉格朗日函数L(x,α)的最优解与其对偶函数的最优解相等，我们就可以将原始的最优化问题转换成为对偶函数的最优化问题。而不难注意到，对于我们的损失函数L(w,b,α)而言，KKT条件都是 可以操作的。如果我们能够人为让KKT条件全部成立，我们就可以求解出L(w,b,α)的对偶函数来解出α。\n",
    "之前我们已经让拉格朗日函数上对参数w和b的求导为0，得到了式子：\n",
    "$$\\sum\\limits_{i=1}^{m}α_iy_ix_i = w \\tag{1}$$\n",
    "$$\\sum\\limits_{i=1}^{m}α_iy_i = 0  \\tag{2}$$\n",
    "并且在我们的函数中，我们通过先求解最大值再求解最小值的方法使得函数天然满足：\n",
    "$$-(y_i(wx_i + b) - 1) \\leq 0 \\tag{3}$$\n",
    "$$α_i \\geq 0 \\tag{4}$$\n",
    "所以接下来，我们只需要再满足一个条件：\n",
    "$$α_i(y_i(wx_i + b) - 1) = 0 \\tag{5}$$\n",
    "\n",
    "这个条件其实很容易满足，能够让$y_i(wx_i + b) - 1=0$的就是落在虚线的超平面上的样本点，即我们的支持向量。所有不是支持向量的样本点则必须满足 $α_i = 0$。满足这个式子说明了，我们求解的参数w和b以及求解的超平面的存在，只与支持向量相关，与其他样本点都无关。现在KKT的五个条件都得到了满足，我们就可以使用L(w,b,α)的对偶函数来求解α了。\n",
    "### 怎样进行转换\n",
    "转换过程如下:\n",
    "$$\\begin{align} \n",
    "L(w,b,α) & =  \\frac{1}{2}||w||_2^2 - \\sum\\limits_{i=1}^{m}\\alpha_i[y_i(w^Tx_i + b) - 1] \\tag{1}\\\\& \n",
    "               = \\frac{1}{2}w^Tw-\\sum\\limits_{i=1}^{m}\\alpha_iy_iw^Tx_i - \\sum\\limits_{i=1}^{m}\\alpha_iy_ib + \\sum\\limits_{i=1}^{m}\\alpha_i \\tag{2}\\\\& \n",
    "               = \\frac{1}{2}w^T\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i -\\sum\\limits_{i=1}^{m}\\alpha_iy_iw^Tx_i - \\sum\\limits_{i=1}^{m}\\alpha_iy_ib + \\sum\\limits_{i=1}^{m}\\alpha_i \\tag{3}\\\\& \n",
    "               = \\frac{1}{2}w^T\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i - w^T\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i - \\sum\\limits_{i=1}^{m}\\alpha_iy_ib + \\sum\\limits_{i=1}^{m}\\alpha_i  \\tag{4}\\\\& \n",
    "               = - \\frac{1}{2}w^T\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i - \\sum\\limits_{i=1}^{m}\\alpha_iy_ib + \\sum\\limits_{i=1}^{m}\\alpha_i  \\tag{5}\\\\& \n",
    "               = - \\frac{1}{2}w^T\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i - b\\sum\\limits_{i=1}^{m}\\alpha_iy_i + \\sum\\limits_{i=1}^{m}\\alpha_i \\tag{6}\\\\& \n",
    "               = -\\frac{1}{2}(\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i)^T(\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i) - b\\sum\\limits_{i=1}^{m}\\alpha_iy_i + \\sum\\limits_{i=1}^{m}\\alpha_i  \\tag{7}\\\\& \n",
    "               = -\\frac{1}{2}\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i^T\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i - b\\sum\\limits_{i=1}^{m}\\alpha_iy_i + \\sum\\limits_{i=1}^{m}\\alpha_i \\tag{8}\\\\& \n",
    "               = -\\frac{1}{2}\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i^T\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i + \\sum\\limits_{i=1}^{m}\\alpha_i \\tag{9}\\\\& \n",
    "               = -\\frac{1}{2}\\sum\\limits_{i=1,j=1}^{m}\\alpha_iy_ix_i^T\\alpha_jy_jx_j + \\sum\\limits_{i=1}^{m}\\alpha_i \\tag{10}\\\\& \n",
    "               = \\sum\\limits_{i=1}^{m}\\alpha_i  - \\frac{1}{2}\\sum\\limits_{i=1,j=1}^{m}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j  \\tag{11}\n",
    "\\end{align}$$\n",
    "其中，(1)式到(2)式用到了范数的定义$||𝑤||^2=𝑤^𝑇𝑤$, (2)式到(3)式用到了上面的$𝑤=\\sum\\limits_{i=1}^{m}α_iy_ix_i$， (3)式到(4)式把和样本无关的$𝑤^𝑇$提前，(4)式到(5)式合并了同类项，(5)式到(6)式把和样本无关的𝑏提前，(6)式到(7)式继续用到$𝑤=\\sum\\limits_{i=1}^{m}α_iy_ix_i$，（7）式到(8)式用到了向量的转置。由于常量的转置是其本身，所有只有向量$x_i$被转置，（8）式到(9)式用到了上面的$\\sum\\limits_{i=1}^{m}\\alpha_iy_i = 0$，（9）式到(10)式使用了$(𝑎+𝑏+𝑐+…)(𝑎+𝑏+𝑐+…)=𝑎𝑎+𝑎𝑏+𝑎𝑐+𝑏𝑎+𝑏𝑏+𝑏𝑐+…$的乘法运算法则，（10）式到(11)式仅仅是位置的调整。\n",
    "将矩阵相乘转换为内积形式： \n",
    "$$L_d=\\sum\\limits_{i=1}^{m}\\alpha_i  - \\frac{1}{2}\\sum\\limits_{i=1,j=1}^{m}\\alpha_i\\alpha_jy_iy_jx_i\\cdot{x_j}$$\n",
    "函数$L_d$就是我们的对偶函数.对所有存在对偶函数的拉格朗日函数有对偶差异如下:\n",
    "$$\\Delta = \\min_xL(x,α)-\\max_αg(α)$$\n",
    "则对于我们的$L(w,b,α)$和$L_d$,有:\n",
    "$$\\Delta = \\min_{w,b}\\; \\max_{α_i \\geq 0} L(w,b,α) - \\max{α_i \\geq 0} L_d$$\n",
    "上面我们推导$L_d$的第一步是对$L(w,b,α)$求偏导并让偏导数都为0,所以我们求解对偶函数的过程其实是在求解$L(w,b,α)$的最小值,所以我们又可以把公式写成:\n",
    "$$\\Delta = \\min_{w,b}\\; \\max_{α_i \\geq 0} L(w,b,α) - \\max_{α_i \\geq 0}\\; \\min_{w,b}L(w,b,α)$$\n",
    "$$\\because 所有KKT条件被满足, \\therefore \\Delta=0$$\n",
    "$$\\therefore \\min_{w,b}\\; \\max_{α_i \\geq 0} L(w,b,α) = \\max_{α_i \\geq 0}\\; \\min_{w,b}L(w,b,α)$$\n",
    "这就是对偶函数和原始函数的变化过程, 最终我们的目标函数变为:\n",
    "$$\\max_{α_i \\geq 0}(\\sum\\limits_{i=1}^{m}\\alpha_i  - \\frac{1}{2}\\sum\\limits_{i=1,j=1}^{m}\\alpha_i\\alpha_jy_iy_jx_i\\cdot{x_j})$$\n",
    "\n",
    "## 求解拉格朗日对偶函数极其后续过程\n",
    "到了这一步，可以使用梯度下降，SMO或者二次规划(QP，quadratic programming)来求解$\\alpha$，数学的难度又进一步上升。考虑到这一过程对数学的要求已经超出了我们需要的程度，也超出我们在使用sklearn时需要掌握的程度，如何求解对偶函数中的$\\alpha$在这里就不做讲解了。   \n",
    "假设我们用SMO算法求出对应的𝛼向量的值$𝛼^∗$向量.   \n",
    "      计算$𝑤^∗=\\sum\\limits_{𝑖=1}^{𝑚}𝛼^∗_𝑖𝑦_𝑖𝑥_𝑖$   \n",
    "找出所有的S个支持向量,即满足$𝛼_𝑠>0$对应的样本$(𝑥_𝑠,𝑦_𝑠)$，通过 $y_s(\\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i^Tx_s+b) = 1$，计算出每个支持向量$(𝑥_𝑥,𝑦_𝑠)$对应的$𝑏^∗_𝑠$,计算出这些$b_s^{*} = y_s - \\sum\\limits_{i=1}^{m}\\alpha_iy_ix_i^Tx_s$. 所有的$b_s^{*}$对应的平均值即为最终的$b^{*} = \\frac{1}{S}\\sum\\limits_{i=1}^{S}b_s^{*}$   \n",
    "这样最终的分类超平面为：$w^{*} \\bullet x + b^{*} = 0$，最终的分类决策函数为：$f(x) = sign(w^{*} \\bullet x + b^{*})$   \n",
    "$sign(h)$是h > 0时返回1，h < 0时返回-1的符号函数。到这里，我们可以说我们完成了对SVM的第二层理解的大部分内容，我们了解了线性SVM的四种相关函数：损失函数的初始形态，拉格朗日函数，拉格朗日对偶函数以及最后的决策函数。\n",
    "\n",
    "# 相关数学知识\n",
    "点到直线的距离公式 $d=\\frac{|ax_0 + by_0 + c|}{\\sqrt{a^2+b^2}}$ 具体到SVM中 $\\gamma = \\frac{y(w^Tx + b)}{||w||_2} =  \\frac{\\gamma^{'}}{||w||_2}$\n",
    "\n",
    "线性代数中模长的运用  \n",
    ">向量b除以自身的模长${||b||}$可以得到b方向上的单位向量.  \n",
    ">向量a乘以b方向上的单位向量,可以得到向量a在向量b方向上的投影的长度.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
